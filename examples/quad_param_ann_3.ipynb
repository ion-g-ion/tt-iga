{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtt as tntt\n",
    "import numpy as np\n",
    "import tt_iga\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as tn\n",
    "import datetime\n",
    "import torch.utils.data \n",
    "\n",
    "tn.set_default_dtype(tn.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_geometry( ):\n",
    "        \n",
    "    Nt = 24                                                                \n",
    "    lz = 40e-3                                                             \n",
    "    Do = 72e-3                                                            \n",
    "    Di = 51e-3                                                            \n",
    "    hi = 13e-3                                                             \n",
    "    bli = 3e-3                                                             \n",
    "    Dc = 3.27640e-2                                                           \n",
    "    # Dc = 32e-3\n",
    "    # hc = 7e-3\n",
    "    hc = 7.55176e-3                                                           \n",
    "    ri = 20e-3                                                           \n",
    "    ra = 18e-3                                                           \n",
    "    blc = hi-hc                                                           \n",
    "    rm = (Dc*Dc+hc*hc-ri*ri)/(Dc*np.sqrt(2)+hc*np.sqrt(2)-2*ri)                 \n",
    "    R = rm-ri\n",
    "\n",
    "    get_rO = lambda Ax,Ay,ri: (Ax**2+Ay**2-ri**2)/(np.sqrt(2)*(Ax+Ay)-2*ri)\n",
    "\n",
    "    O = np.array([rm/np.sqrt(2),rm/np.sqrt(2)])\n",
    "    alpha1 = -np.pi*3/4       \n",
    "    alpha2 = np.math.asin((hc-rm/np.sqrt(2))/R)\n",
    "    alpha = np.abs(alpha2-alpha1)\n",
    "    \n",
    "    A = np.array([[O[0] - ri/np.sqrt(2), O[1] - ri/np.sqrt(2)], [O[0] - Dc, O[1] - hc]])\n",
    "    b = np.array([[A[0,0]*ri/np.sqrt(2)+A[0,1]*ri/np.sqrt(2)],[A[1,0]*Dc+A[1,1]*hc]])\n",
    "    C = np.linalg.solve(A,b)\n",
    "\n",
    "    def Cx(ri,Dc,hc):\n",
    "        O = [((Dc*Dc+hc*hc-ri*ri)/(Dc*np.sqrt(2)+hc*np.sqrt(2)-2*ri) )/np.sqrt(2),((Dc*Dc+hc*hc-ri*ri)/(Dc*np.sqrt(2)+hc*np.sqrt(2)-2*ri) )/np.sqrt(2)]\n",
    "        A = np.array([[O[0] - ri/np.sqrt(2), O[1] - ri/np.sqrt(2)], [O[0] - Dc, O[1] - hc]])\n",
    "        b = np.array([[A[0,0]*ri/np.sqrt(2)+A[0,1]*ri/np.sqrt(2)],[A[1,0]*Dc+A[1,1]*hc]])\n",
    "        C = np.linalg.solve(A,b)\n",
    "        return C[0]\n",
    "\n",
    "    def Cy(ri,Dc,hc):\n",
    "        O = [((Dc*Dc+hc*hc-ri*ri)/(Dc*np.sqrt(2)+hc*np.sqrt(2)-2*ri) )/np.sqrt(2),( (Dc*Dc+hc*hc-ri*ri)/(Dc*np.sqrt(2)+hc*np.sqrt(2)-2*ri)  )/np.sqrt(2)]\n",
    "        A = np.array([[O[0] - ri/np.sqrt(2), O[1] - ri/np.sqrt(2)], [O[0] - Dc, O[1] - hc]])\n",
    "        b = np.array([[A[0,0]*ri/np.sqrt(2)+A[0,1]*ri/np.sqrt(2)],[A[1,0]*Dc+A[1,1]*hc]])\n",
    "        C = np.linalg.solve(A,b)\n",
    "        return C[1]\n",
    "\n",
    "    control_points = tt_iga.geometry.ParameterDependentControlPoints([7,5])\n",
    "\n",
    "    # params ri - 0 , Dc - 1 , Di - 2 \n",
    "    control_points[:,0,0] = [0,0]\n",
    "    control_points[:,1,0] = [lambda params: (Dc+params[1])/2, 0]\n",
    "    control_points[:,2,0] = [lambda params: (Dc+params[1]), 0]\n",
    "    control_points[:,3,0] = [(Dc+Di)/2,0]\n",
    "    control_points[:,4,0] = [lambda params: Di+params[2],0]\n",
    "    control_points[:,5,0] = [(Do+Di)/2,0]\n",
    "    control_points[:,6,0] = [Do,0]\n",
    "    \n",
    "    control_points[:,0,1] = [lambda params: (ri+params[0])/np.sqrt(2)*0.5,lambda params: (ri+params[0])/np.sqrt(2)*0.5]\n",
    "    control_points[:,1,1] = [lambda params: (Cx(ri,Dc,hc)+Dc/2)*0.5, lambda params: Cy(ri,Dc,hc)*0.5]\n",
    "    control_points[:,2,1] = [lambda params: (Dc+params[1]), hc*0.5]\n",
    "    control_points[:,3,1] = [(Dc+Di)/2,0.5*(hi-bli+hc)*0.5]\n",
    "    control_points[:,4,1] = [lambda params: Di+params[2],(hi-bli)*0.5]\n",
    "    control_points[:,5,1] = [(Do+Di)/2,(hi-bli)*0.5]\n",
    "    control_points[:,6,1] = [Do,(hi-bli)*0.5]\n",
    "\n",
    "    #control_points[:,0,1] = [lambda params: (ri+params[0])/np.sqrt(2)*0.5,lambda params: (ri+params[0])/np.sqrt(2)*0.5]\n",
    "    #control_points[:,1,1] = [lambda params: (Cx(ri+params[0],Dc+params[1],hc)+(Dc+params[1])/2)*0.5, lambda params: Cy(ri+params[0],Dc+params[1],hc)*0.5]\n",
    "    #control_points[:,2,1] = [lambda params: (Dc+params[1]), hc*0.5]\n",
    "    #control_points[:,3,1] = [(Dc+Di)/2,0.5*(hi-bli+hc)*0.5]\n",
    "    #control_points[:,4,1] = [lambda params: Di+params[2],(hi-bli)*0.5]\n",
    "    #control_points[:,5,1] = [(Do+Di)/2,(hi-bli)*0.5]\n",
    "    #control_points[:,6,1] = [Do,(hi-bli)*0.5]\n",
    "    \n",
    "    control_points[:,0,2] = [lambda params: (ri+params[0])/np.sqrt(2),lambda params: (ri+params[0])/np.sqrt(2)]\n",
    "    control_points[:,1,2] = [lambda params: Cx(ri+params[0],Dc+params[1],hc), lambda params: Cy(ri+params[0],Dc+params[1],hc)]\n",
    "    control_points[:,2,2] = [lambda params: (Dc+params[1]), hc]\n",
    "    control_points[:,3,2] = [(Dc+Di)/2,0.5*(hi-bli+hc)]\n",
    "    control_points[:,4,2] = [lambda params: Di+params[2],hi-bli]\n",
    "    control_points[:,5,2] = [(Do+Di)/2,hi-bli]\n",
    "    control_points[:,6,2] = [Do,hi-bli]\n",
    "    \n",
    "    control_points[:,0,3] = [(0.75*ri+0.25*Do)/np.sqrt(2),(0.75*ri+0.25*Do)/np.sqrt(2)]\n",
    "    control_points[:,1,3] = [0.5*((0.75*ri+0.25*Do)/np.sqrt(2)+Dc+blc), 0.5*((0.75*ri+0.25*Do)/np.sqrt(2)+hi)]\n",
    "    control_points[:,2,3] = [Dc+blc,lambda params:  hi+params[3]]\n",
    "    control_points[:,3,3] = [ 0.5*(Dc+blc+Di-bli),lambda params:  hi+params[3]]\n",
    "    control_points[:,4,3] = [ Di-bli, lambda params: hi+params[3]]\n",
    "    control_points[:,5,3] = [lambda params: 0.5*(Di+Do),hi]\n",
    "    control_points[:,6,3] = [Do,hi]\n",
    "\n",
    "    control_points[:,0,4] = [Do/np.sqrt(2), Do/np.sqrt(2)]\n",
    "    control_points[:,1,4] = [Do/np.sqrt(2)*(1-0.5*Dc/Do)+Do*(0.5*Dc/Do), Do/np.sqrt(2)*(1-0.5*Dc/Do)+Do*np.tan(np.pi/8)*(0.5*Dc/Do)]\n",
    "    control_points[:,2,4] = [Do/np.sqrt(2)*(1-Dc/Do)+Do*(Dc/Do), Do/np.sqrt(2)*(1-Dc/Do)+Do*np.tan(np.pi/8)*(Dc/Do)]\n",
    "    control_points[:,3,4] = [Do/np.sqrt(2)*(1-0.5*(Dc+Di)/Do)+Do*(0.5*(Dc+Di)/Do), Do/np.sqrt(2)*(1-0.5*(Dc+Di)/Do)+Do*np.tan(np.pi/8)*(0.5*(Dc+Di)/Do)]\n",
    "    control_points[:,4,4] = [Do/np.sqrt(2)*(1-Di/Do)+Do*(Di/Do), Do/np.sqrt(2)*(1-Di/Do)+Do*np.tan(np.pi/8)*(Di/Do)]\n",
    "    control_points[:,5,4] = [Do/np.sqrt(2)*(1-0.5*(Di+Do)/Do)+Do*(0.5*(Di+Do)/Do), Do/np.sqrt(2)*(1-0.5*(Di+Do)/Do)+Do*np.tan(np.pi/8)*(0.5*(Di+Do)/Do)]\n",
    "    control_points[:,6,4] = [Do,Do*np.tan(np.pi/8)]\n",
    "\n",
    "    weights = tt_iga.geometry.ParameterDependentWeights([7,5])\n",
    "    weights[...] = 1.0\n",
    "    alpha2 = lambda params: np.math.asin((hc-get_rO(Dc+params[1],hc,ri+params[0])/np.sqrt(2))/(get_rO(Dc+params[1],hc,ri+params[0])-ri-params[0]))\n",
    "    weights[1,2] = lambda params :  np.sin((np.pi-np.abs(alpha2(params)+np.pi*3/4))/2)\n",
    "    # weights[1,2] = lambda params :  np.abs(alpha2+np.pi*3/4)\n",
    "    \n",
    "    surface_excitation = lambda params: 0.5*(blc-params[...,1])*(hi+params[...,3]+hc)+(Di-bli-Dc-blc)*(hi+params[...,3])+0.5*(bli+params[...,2])*(hi+params[...,3]+hi-bli)\n",
    "    return control_points, weights, surface_excitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_points, weights, surface = create_geometry()\n",
    "\n",
    "var1, var2, var3, var4 = (0.00075, 0.00075, 0.00075, 0.00075)\n",
    "\n",
    "basis1 = tt_iga.bspline.BSplineBasis(np.array([0,0.4,0.4,0.6,0.6,1]),2)\n",
    "basis2 = tt_iga.bspline.BSplineBasis(np.array([0,0.15,0.3,0.5,1]),1)\n",
    "basis = [basis1, basis2]\n",
    "basis_solution = [tt_iga.bspline.BSplineBasis(np.concatenate((np.linspace(0,0.4,21), np.linspace(0.4,0.6,21),np.linspace(0.6,1,21))),2)]\n",
    "basis_solution.append(tt_iga.bspline.BSplineBasis(np.concatenate((np.linspace(0,0.15,16),np.linspace(0.15,0.3,15), np.linspace(0.3,0.5,15),np.linspace(0.5,1,17))),2))\n",
    "\n",
    "nl = 10\n",
    "basis_param = [tt_iga.lagrange.LagrangeLeg(nl,[-var1,var1]), tt_iga.lagrange.LagrangeLeg(nl,[-var2,var2]), tt_iga.lagrange.LagrangeLeg(nl,[-var3,var3]), tt_iga.lagrange.LagrangeLeg(nl,[-var4,var4])]\n",
    "geom = tt_iga.PatchNURBS.interpolate_parameter_dependent(control_points, weights, basis, basis_param, eps = 1e-13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time mass  0:00:01.845289\n",
      "Sweep 1: \n",
      "\tLR supercore 1,2\n",
      "\t\tnumber evaluations 113280\n",
      "\t\trank updated 2 -> 6, local error 1.000225e+00\n",
      "\tLR supercore 2,3\n",
      "\t\tnumber evaluations 28320\n",
      "\t\trank updated 2 -> 4, local error 1.141155e+00\n",
      "\tLR supercore 3,4\n",
      "\t\tnumber evaluations 800\n",
      "\t\trank updated 2 -> 4, local error 1.022674e+00\n",
      "\tLR supercore 4,5\n",
      "\t\tnumber evaluations 800\n",
      "\t\trank updated 2 -> 4, local error 1.054715e+00\n",
      "\tLR supercore 5,6\n",
      "\t\tnumber evaluations 400\n",
      "\t\trank updated 2 -> 4, local error 7.196365e-01\n",
      "\tRL supercore 5,6\n",
      "\t\tnumber evaluations 400\n",
      "\t\trank updated 4 -> 4, local error 6.215804e-16\n",
      "\tRL supercore 4,5\n",
      "\t\tnumber evaluations 1600\n",
      "\t\trank updated 4 -> 4, local error 3.631068e-15\n",
      "\tRL supercore 3,4\n",
      "\t\tnumber evaluations 1600\n",
      "\t\trank updated 4 -> 4, local error 1.733806e-15\n",
      "\tRL supercore 2,3\n",
      "\t\tnumber evaluations 56640\n",
      "\t\trank updated 4 -> 4, local error 1.442112e-15\n",
      "\tRL supercore 1,2\n",
      "\t\tnumber evaluations 226560\n",
      "\t\trank updated 6 -> 6, local error 8.852514e-15\n",
      "Max error 1.14115\n",
      "Sweep 2: \n",
      "\tLR supercore 1,2\n",
      "\t\tnumber evaluations 226560\n",
      "\t\trank updated 6 -> 6, local error 6.324972e-15\n",
      "\tLR supercore 2,3\n",
      "\t\tnumber evaluations 56640\n",
      "\t\trank updated 4 -> 4, local error 1.484187e-14\n",
      "\tLR supercore 3,4\n",
      "\t\tnumber evaluations 1600\n",
      "\t\trank updated 4 -> 4, local error 4.348577e-15\n",
      "\tLR supercore 4,5\n",
      "\t\tnumber evaluations 1600\n",
      "\t\trank updated 4 -> 4, local error 7.887442e-16\n",
      "\tLR supercore 5,6\n",
      "\t\tnumber evaluations 400\n",
      "\t\trank updated 4 -> 4, local error 7.520548e-16\n",
      "\tRL supercore 5,6\n",
      "\t\tnumber evaluations 400\n",
      "\t\trank updated 4 -> 4, local error 3.541725e-16\n",
      "\tRL supercore 4,5\n",
      "\t\tnumber evaluations 1600\n",
      "\t\trank updated 4 -> 4, local error 6.740942e-16\n",
      "\tRL supercore 3,4\n",
      "\t\tnumber evaluations 1600\n",
      "\t\trank updated 4 -> 4, local error 1.143962e-15\n",
      "\tRL supercore 2,3\n",
      "\t\tnumber evaluations 56640\n",
      "\t\trank updated 4 -> 4, local error 2.779509e-15\n",
      "\tRL supercore 1,2\n",
      "\t\tnumber evaluations 226560\n",
      "\t\trank updated 6 -> 6, local error 8.225551e-15\n",
      "Max error 1.484187e-14 < 1.000000e-10  ---->  DONE\n",
      "number of function calls  1004000\n",
      "\n",
      "Time stiffness  0:00:42.972257\n",
      "System matrix... \n",
      "Rank Stiff [1, 93, 87, 61, 26, 7, 1]  storage [MB]  273.838912\n",
      "Rank Mass [1, 18, 22, 11, 4, 2, 1]  storage [MB]  13.802752\n",
      "Rank Mtt  [1, 92, 85, 60, 26, 7, 1]  storage [MB]  264.739616\n",
      "Rank rhstt  [1, 3, 6, 6, 10, 4, 1]  storage [MB]  0.021952\n"
     ]
    }
   ],
   "source": [
    "mu0 = 0.001\n",
    "mur = 1000\n",
    "mu_ref = lambda y: 1/mu0*((y[...,1]<0.5)*(y[...,0]<0.6)*(y[...,0]>0.4)+(y[...,1]<0.3)*(y[...,0]<0.4))+1/(mu0*mur)*tn.logical_not((y[...,1]<0.5)*(y[...,0]<0.6)*(y[...,0]>0.4)+(y[...,1]<0.3)*(y[...,0]<0.4))\n",
    "\n",
    "tme = datetime.datetime.now()\n",
    "Mass_tt = geom.mass_interp(basis_solution, eps = 1e-10)\n",
    "tme = datetime.datetime.now() - tme\n",
    "print('Time mass ', tme)\n",
    "tme = datetime.datetime.now()\n",
    "Stiff_tt = geom.stiffness_interp(basis_solution, eps = 1e-10, func_reference=mu_ref, qtt = True, verb = False, device = 'cuda:0')\n",
    "tme = datetime.datetime.now() - tme\n",
    "print('Time stiffness ', tme)\n",
    "\n",
    "Jref = lambda y: 24*45/surface(y[...,2:])*(y[...,1]<0.5)*(y[...,0]<0.6)*(y[...,0]>0.4)+0.0\n",
    "# Jref = lambda y: 5000000*(y[...,1]<0.5)*(y[...,0]<0.6)*(y[...,0]>0.4)+0.0\n",
    "\n",
    "rhs_tt = geom.rhs_interp(basis_solution,Jref)\n",
    "\n",
    "P1 = tn.eye(Mass_tt.N[0])\n",
    "P2 = tn.eye(Mass_tt.N[1])\n",
    "P2[-1,-1] = 0\n",
    "P1[0,0] = 0\n",
    "P1[-1,-1] = 0\n",
    "Pin_tt = tntt.rank1TT([P1,P2]) ** tntt.eye([nl]*4)\n",
    "Pbd_tt = (tntt.eye(Mass_tt.N) - Pin_tt) ** tntt.eye([nl]*4) / Mass_tt.N[0]\n",
    "\n",
    "M_tt = (Pin_tt@Stiff_tt+Pbd_tt).round(1e-10)\n",
    "rhs_tt = (Pin_tt @ rhs_tt + 0).round(1e-10)\n",
    "\n",
    "print('System matrix... ',flush=True)\n",
    "\n",
    "print('Rank Stiff', Stiff_tt.R, ' storage [MB] ',tntt.numel(Stiff_tt)*8/1e6)\n",
    "print('Rank Mass', Mass_tt.R, ' storage [MB] ',tntt.numel(Mass_tt)*8/1e6)\n",
    "print('Rank Mtt ',M_tt.R, ' storage [MB] ',tntt.numel(M_tt)*8/1e6)\n",
    "print('Rank rhstt ',rhs_tt.R, ' storage [MB] ',tntt.numel(rhs_tt)*8/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tme = datetime.datetime.now() \n",
    "# dev_name = 'cuda:0'\n",
    "# dofs_tt = tntt.solvers.amen_solve(M_tt.to(dev_name), rhs_tt.to(dev_name), x0 = tntt.ones(rhs_tt.N).to(dev_name), rmax = 300, eps = 1e-6, nswp = 50, kickrank = 8, preconditioner = 'c', verbose = False).cpu()\n",
    "# tme = datetime.datetime.now() - tme\n",
    "# print('Time system solve ',tme,flush=True)\n",
    "# fspace = tt_iga.Function(basis_solution+basis_param)\n",
    "# fspace.dofs = dofs_tt.round(1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "device_name = 'cuda:1'\n",
    "\n",
    "def generate_batch(n):\n",
    "   # tn.manual_seed(123)\n",
    "    indices = tn.randint(nl,(n,4))*0\n",
    "    inputs = tn.zeros((n,4)) # tn.rand((n,4))*2-1\n",
    "    operators = []\n",
    "    rhss = []\n",
    "    varz = [var1,var2,var3,var4]\n",
    "\n",
    "    for k,i in enumerate(indices):\n",
    "        print(k)\n",
    "        inputs[k,:] = tn.tensor([basis_param[l].interpolating_points()[0][i[l]]/varz[l] for l in range(4)])\n",
    "        i = np.array(i)\n",
    "        \n",
    "        operators.append(M_tt[:,:,int(i[0]),int(i[1]),int(i[2]),int(i[3]),:,:,int(i[0]), int(i[1]), int(i[2]), int(i[3])].round(1e-12).to(device_name))\n",
    "        rhss.append(rhs_tt[:,:,int(i[0]),int(i[1]),int(i[2]),int(i[3])].to(device_name))\n",
    "\n",
    "    return inputs, np.array(operators), np.array(rhss)\n",
    "\n",
    "n_train = 1000\n",
    "train_inputs, train_operators,  train_rhs = generate_batch(n_train)\n",
    "#train_data = torch.utils.data.TensorDataset(train_ins, train_outs)\n",
    "\n",
    "n_test = 100\n",
    "test_inputs, test_operators, test_rhs = generate_batch(n_test)\n",
    "#test_data = torch.utils.data.TensorDataset(test_ins, test_outs)\n",
    "\n",
    "#dataloader_train = tn.utils.data.DataLoader(train_data, batch_size=100, shuffle=True, num_workers=10, pin_memory = False)\n",
    "#dataloader_test = tn.utils.data.DataLoader(test_data, batch_size=100, shuffle=True, num_workers=10, pin_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPINN(nn.Module):\n",
    "   def __init__(self):\n",
    "      super(CPINN, self).__init__()\n",
    "      np = 4\n",
    "      ngf = 8\n",
    "      self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( np, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, 1, 4, 2, 1, bias=True),\n",
    "            \n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "      \n",
    "   def forward(self, x):\n",
    "      return self.main(x.view([-1,4,1,1])).view([-1,64,64])\n",
    "\n",
    "class ANN(nn.Module):\n",
    "   def __init__(self):\n",
    "      super(ANN, self).__init__()\n",
    "      self.fc1 = nn.Linear(4, 25)\n",
    "      self.fc2 = nn.Linear(25, 25)\n",
    "      self.fc3 = nn.Linear(25, 64*64)\n",
    "   def forward(self, x):\n",
    "      x = tn.tanh(self.fc1(x))\n",
    "      x = tn.tanh(self.fc2(x))\n",
    "      x = (self.fc3(x))\n",
    "      return x.view([-1,64,64])\n",
    "\n",
    "class TTN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        p = 0.25\n",
    "        self.fc1 = nn.Linear(4, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.ttl1 = tntt.nn.LinearLayerTT([2,2,2,2], [4,4,4,4], [1,4,4,4,1], initializer = 'He', dtype = tn.float64)\n",
    "        self.dropout1 = nn.Dropout(p)\n",
    "        self.ttl2 = tntt.nn.LinearLayerTT([4,4,4,4], [6,6,6,6], [1,4,4,4,1], initializer = 'He', dtype = tn.float64)\n",
    "        self.dropout2 = nn.Dropout(p)\n",
    "        self.ttl3 = tntt.nn.LinearLayerTT([6,6,6,6], [8,8,8,8], [1,4,4,4,1], initializer = 'He', dtype = tn.float64)\n",
    "        self.dropout3 = nn.Dropout(p)\n",
    "        self.ttl4 = tntt.nn.LinearLayerTT([8,8,8,8], [8,8,8,8], [1,2,2,2,1], initializer = 'He', dtype = tn.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = tn.tanh(self.fc1(x))\n",
    "        x = tn.tanh(self.fc2(x))\n",
    "        x = x.view([-1,2,2,2,2])\n",
    "        \n",
    "        x = self.ttl1(x)\n",
    "        #x = self.dropout1(x)\n",
    "        x = tn.relu(x)\n",
    "\n",
    "        x = self.ttl2(x)\n",
    "        #x = self.dropout2(x)\n",
    "        x = tn.relu(x)\n",
    "\n",
    "        x = self.ttl3(x)\n",
    "        #x = self.dropout3(x)\n",
    "        x = tn.relu(x)\n",
    "\n",
    "        x = self.ttl4(x)\n",
    "        \n",
    "        return x.view([-1,64,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPINN(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(4, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tn.cuda.empty_cache() \n",
    "\n",
    "model = CPINN()\n",
    "print(model)\n",
    "model.to(device_name) #.to(tn.float32)\n",
    "\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400: iterations 1, loss train 8.280194e-01, loss test 0.000000e+00\n",
      "Epoch 2/400: iterations 1, loss train 7.804219e-01, loss test 0.000000e+00\n",
      "Epoch 3/400: iterations 1, loss train 6.373659e-01, loss test 0.000000e+00\n",
      "Epoch 4/400: iterations 1, loss train 6.274976e-01, loss test 0.000000e+00\n",
      "Epoch 5/400: iterations 1, loss train 6.175404e-01, loss test 0.000000e+00\n",
      "Epoch 6/400: iterations 1, loss train 5.743841e-01, loss test 0.000000e+00\n",
      "Epoch 7/400: iterations 1, loss train 5.485262e-01, loss test 0.000000e+00\n",
      "Epoch 8/400: iterations 1, loss train 5.479517e-01, loss test 0.000000e+00\n",
      "Epoch 9/400: iterations 1, loss train 5.539891e-01, loss test 0.000000e+00\n",
      "Epoch 10/400: iterations 1, loss train 5.504350e-01, loss test 0.000000e+00\n",
      "Epoch 11/400: iterations 1, loss train 5.441467e-01, loss test 0.000000e+00\n",
      "Epoch 12/400: iterations 1, loss train 5.346273e-01, loss test 0.000000e+00\n",
      "Epoch 13/400: iterations 1, loss train 5.244326e-01, loss test 0.000000e+00\n",
      "Epoch 14/400: iterations 1, loss train 5.216376e-01, loss test 0.000000e+00\n",
      "Epoch 15/400: iterations 1, loss train 5.229747e-01, loss test 0.000000e+00\n",
      "Epoch 16/400: iterations 1, loss train 5.231052e-01, loss test 0.000000e+00\n",
      "Epoch 17/400: iterations 1, loss train 5.226433e-01, loss test 0.000000e+00\n",
      "Epoch 18/400: iterations 1, loss train 5.224119e-01, loss test 0.000000e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113777/3038522683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Make predictions for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;31m#loss += 0.5*tn.sum(out * (op @ out)) - tn.sum(rs.full() * out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_113777/1697415060.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mANN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[has-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = tn.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "optimizer = tn.optim.Adam(model.parameters(), lr=0.010)\n",
    "scheduler = tn.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "model.to(device_name).to(tn.float64)\n",
    "\n",
    "tn.cuda.empty_cache() \n",
    "\n",
    "n_epochs = 400\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "history_test_loss = []\n",
    "history_train_loss = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    loss_train = 0\n",
    "\n",
    "    perm = np.random.permutation(n_train)\n",
    "    batch_idx = np.split(perm,np.arange(1,n_train//batch_size)*batch_size)\n",
    "\n",
    "    model.train(True)\n",
    "\n",
    "    for bi in batch_idx:\n",
    "        \n",
    "        params = train_inputs[list(bi),:].to(device_name)\n",
    "        ops = train_operators[bi]\n",
    "        rhss = train_rhs[bi]\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        for op, rs, param in zip(ops, rhss, params):\n",
    "            # Make predictions for this batch\n",
    "            out = model(param)[0,:,:]\n",
    "            #loss += 0.5*tn.sum(out * (op @ out)) - tn.sum(rs.full() * out)\n",
    "            loss += 0.5*tn.linalg.norm( op @ out - rs.full())/tn.linalg.norm(rs.full())\n",
    "        loss /= batch_size\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_train += loss\n",
    "        optimizer.step()\n",
    "    loss_train /= n_train//batch_size\n",
    "    model.train(False)\n",
    "    \n",
    "\n",
    "    loss_test = 0\n",
    "    # for i,(param,sol) in enumerate(dataloader_train):\n",
    "    #     \n",
    "    #     param = param.to(device_name).to(tn.float32)\n",
    "    #     sol = sol.to(device_name).to(tn.float32)\n",
    "    #     \n",
    "    #     \n",
    "    #     # Make predictions for this batch\n",
    "    #     out = model(param)\n",
    "    #     # Compute the loss and its gradients\n",
    "    #     loss = tn.sqrt(loss_function(out, sol))\n",
    "    #     # regularization\n",
    "    #     l2_lambda = 0.00005\n",
    "    #     l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "    #     loss = loss+0#l2_lambda*l2_norm\n",
    "    #     loss_test += loss\n",
    "\n",
    "\n",
    "    print('Epoch %d/%d: iterations %d, loss train %e, loss test %e'%(epoch+1, n_epochs, n_train//batch_size, loss_train, loss_test))\n",
    "    history_train_loss.append(loss_train)\n",
    "    history_test_loss.append(loss_test)\n",
    "\n",
    "    tn.cuda.empty_cache() \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f0ea7e86af0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = model(-0.9739*tn.ones((1,4)).to(device_name).to(tn.float64)).cpu()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(n[0,...].detach())\n",
    "plt.colorbar()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9739, -0.9739, -0.9739, -0.9739],\n",
       "        [-0.9739, -0.9739, -0.9739, -0.9739],\n",
       "        [-0.9739, -0.9739, -0.9739, -0.9739],\n",
       "        ...,\n",
       "        [-0.9739, -0.9739, -0.9739, -0.9739],\n",
       "        [-0.9739, -0.9739, -0.9739, -0.9739],\n",
       "        [-0.9739, -0.9739, -0.9739, -0.9739]], device='cuda:1')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ tn.numel(t) for t in list(model.parameters())])\n",
    "train_inputs.min()\n",
    "params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df6fc3a9b7a9c6f4b0308ab6eb361a4cabbf6b5db181383d07014ff4304e5cb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
